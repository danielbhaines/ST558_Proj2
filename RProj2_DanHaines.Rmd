---
title: "R Project 2"
output: github_document
params:
      day: 'Monday'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
library(caret)
library(rattle)
library(rpart)
library(gbm)
library(plyr)
library(rmarkdown)
library(tidyverse)
```

## Data  
```{r, include=FALSE}
# import the data set using a relative path
news <- read.csv('OnlineNewsPopularity.csv')

# recast days of the week as a single variable
news$day <- names(news)[32:38][max.col(news[32:38])]
news$day[news$day=='weekday_is_monday'] <- 'Monday'
news$day[news$day=='weekday_is_tuesday'] <- 'Tuesday'
news$day[news$day=='weekday_is_wednesday'] <- 'Wednesday'
news$day[news$day=='weekday_is_thursday'] <- 'Thursday'
news$day[news$day=='weekday_is_friday'] <- 'Friday'
news$day[news$day=='weekday_is_saturday'] <- 'Saturday'
news$day[news$day=='weekday_is_sunday'] <- 'Sunday'

# recast data channel as a single variable
news$data_channel <- names(news)[14:19][max.col(news[14:19])]
news$data_channel[news$data_channel=='data_channel_is_lifestyle'] <- 'Lifestyle'
news$data_channel[news$data_channel=='data_channel_is_entertainment'] <- 'Entertainment'
news$data_channel[news$data_channel=='data_channel_is_bus'] <- 'Business'
news$data_channel[news$data_channel=='data_channel_is_socmed'] <- 'Social Media'
news$data_channel[news$data_channel=='data_channel_is_tech'] <- 'Tech'
news$data_channel[news$data_channel=='data_channel_is_world'] <- 'World'
news$data_channel <- as.factor(news$data_channel)
news$is_weekend <- as.factor(news$is_weekend)

# subset for a particular day of the week
news <- filter(news, day==params$day)

news <- subset(news, select=c(data_channel, num_imgs, num_videos, title_sentiment_polarity,
                              shares, n_tokens_content))

# split the data into training and test sets
news_index <- createDataPartition(news$shares, p=0.7, list=FALSE)

news_train <- news[news_index,]
news_test <- news[-news_index,]
```

## Summarizations  
```{r}
# plot a visual of shares per article
plot(news_train$shares)
```
  

From the above plot, we can see any outliers. These could be affected by things beyond the variables we are observing in this model.  

```{r}
# correlation plots of the variables selected
pairs(news_train[,2:5])
```
  

From the above plot, we can see any potential trends between the selected variables.

```{r}
# summary statistics of the variables selected
mean(news_train$shares)
median(news_train$shares)
sd(news_train$shares)
```
Above is the mean and standard deviation of the response variable, `shares`. The closeness of the mean to the median is a very basic indicator that outliers are possible.

```{r}
histogram(news_train$shares, breaks=50)
```
  

A histogram displaying the spread of our response variable can be seen above.  

## Modeling  
```{r, cache=TRUE}
# create a non-ensemble tree model chosen using leave one out cross-validation
news_tree <- train(shares ~ .,
                   data=news_train,
                   method='rpart',
                   metric='MAE',
                   tuneLength=10,
                   trControl=trainControl(method='LOOCV'),
                   control=rpart.control(minsplit=1, minbucket=1)
                   )

# print out the results from our model selection
print(news_tree)

# display a plot of the complexity values
plot(news_tree)

# display a plot of the final tree model
fancyRpartPlot(news_tree$finalModel)

# compare model to the test set
tree_pred <- predict(news_tree, newdata=news_test)
postResample(tree_pred, news_test$shares)
```
Above we are fitting a tree-based model using leave one out cross validation. This is a regression tree, meaning we are trying to model a continuous response as opposed to classifying group membership. To fit this, for every possible value of each predictor, the MAE is found and minimized.

A potential issue with a tree-based method is that it can overfit our data, resulting in poorer performance when predicting on new data. This can be improved by pruning and cross-validation.

From the last output, we can see the mean absolute error (MAE). Because both models are using MAE as a means of optimization, we will compare this value to that received from boosting.

```{r, results='hide', cache=TRUE}
# create a boosted tree model using cross-validation
boosted_tree <- train(shares ~ .,
                    data=news_train,
                    method='gbm',
                    metric='MAE',
                    tuneLength=10,
                    trControl=trainControl(method='repeatedcv', number=10, repeats=3)
                    )

```

```{r}
# print out the results from our model selection
print(boosted_tree)

# display a plot of the complexity values
plot(boosted_tree)

# compare model to the test set
boosted_pred <- predict(boosted_tree, newdata=news_test)
postResample(boosted_pred, news_test$shares)
```
A boosted tree model is an ensemble method, meaning it uses multiple learning algorithms to attempt to gain better performance than an individual model. The above model output and plot show the number of trees used in the final model and the tree depth. 

This model performs better than the single tree-based model above in terms of out parameter of interest, MAE. This is likely due to a combination of the fact that the second model is using an ensemble method that fits more slowly, and the fact that singular tree-based models have a tendency to overfit their data, thus affecting the overall prediction value.

```{r linear model}
# create a linear  model using repeated cross-validation
linear_regression <- train(shares ~ .,
                     data=news_train,
                     method='lm',
                     preProcess=c("center", "scale"),
                     metric='MAE',
                     tuneLength=10,
                     trControl=trainControl(method='repeatedcv', number=10, repeats=10)
                     )

linear_regression

linear_pred <- predict(linear_regression, newdata=news_test)
postResample(linear_pred, news_test$shares)
```

